{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retinaface insightface执行人脸识别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys    ##ccrujing\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath('__file__'))))   # face\n",
    "sys.path.append(os.path.join(os.path.dirname(os.path.dirname(os.path.abspath('__file__'))),'retinaface'))\n",
    "sys.path.append(os.path.join(os.path.dirname(os.path.dirname(os.path.abspath('__file__'))),'insightface'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# face detect\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"1\"\n",
    "import argparse\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "from retinaface.data import cfg_mnet,cfg_re50\n",
    "from retinaface.layers.functions.prior_box import PriorBox\n",
    "from retinaface.utils.nms.py_cpu_nms import py_cpu_nms\n",
    "from retinaface.utils.box_utils import decode, decode_landm\n",
    "from retinaface.models.retinaface import RetinaFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# face embedding\n",
    "\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from multiprocessing import Process, Pipe,Value,Array\n",
    "import torch\n",
    "from config import get_config\n",
    "from Learner import face_learner\n",
    "from insight_utils import load_facebank, draw_box_name, prepare_facebank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect 超参数\n",
    "trained_model='retinaface/weights/mobilenet0.25_Final.pth'\n",
    "network='mobile0.25'          #'resnet50'\n",
    "cpu=False\n",
    "confidence_threshold=0.02\n",
    "top_k=5000\n",
    "nms_threshold=0.4\n",
    "keep_top_k=750\n",
    "save_image=False\n",
    "vis_thres=0.6\n",
    "\n",
    "# embedding 超参数\n",
    "prepare_image_path = '/Data/jing/face/facebank'  # facebank路径\n",
    "\n",
    "file_name ='video.mp4'   #\", help=\"video file name\",default='video.mp4', type=str)\n",
    "save_name  ='video_out.avi'  #\", help=\"output file name\",default='recording', type=str)\n",
    "threshold=1.54       #',help='threshold to decide identical faces',default=1.54, type=float)\n",
    "update=True           #\", help=\"whether perform update the facebank\",action=\"store_true\")\n",
    "tta =False           #\", help=\"whether test time augmentation\",action=\"store_true\")\n",
    "score=False         #\", help=\"whether show the confidence score\",action=\"store_true\")\n",
    "begin=0                #\", help=\"from when to start detection(in seconds)\", default=0, type=int)\n",
    "duration=0            #\", help=\"perform detection for how long(in seconds)\", default=0, type=int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data_path': PosixPath('insight_data'), 'work_path': PosixPath('work_space'), 'model_path': PosixPath('work_space/models'), 'log_path': PosixPath('work_space/log'), 'save_path': PosixPath('/home/create/jing/jing_vision/face/insightface/work_space/save'), 'input_size': [112, 112], 'embedding_size': 512, 'use_mobilfacenet': True, 'net_depth': 50, 'drop_ratio': 0.6, 'net_mode': 'ir_se', 'device': device(type='cuda', index=0), 'test_transform': Compose(\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
      "), 'data_mode': 'emore', 'vgg_folder': PosixPath('insight_data/faces_vgg_112x112'), 'ms1m_folder': PosixPath('insight_data/faces_ms1m_112x112'), 'emore_folder': PosixPath('/Data/jing/face/faces_emore'), 'batch_size': 100, 'facebank_path': PosixPath('insight_data/facebank'), 'threshold': 1.5, 'face_limit': 10, 'min_face_size': 30}\n",
      "MobileFaceNet model generated\n",
      "load gpu model ...\n",
      "learner loaded\n"
     ]
    }
   ],
   "source": [
    "# 配置embedding 模型\n",
    "conf = get_config(False)\n",
    "conf.use_mobilfacenet = True   # 是否使用mobilenet模型  \n",
    "learner = face_learner(conf, True)\n",
    "learner.threshold = threshold\n",
    "if conf.device.type == 'cpu':\n",
    "    print('load cpu model ...')\n",
    "    learner.load_state(conf, 'cpu_final.pth', True, True)\n",
    "else:\n",
    "    print('load gpu model ...')\n",
    "    learner.load_state(conf, 'final_mobile.pth', True, True)  # 加载指定路径下 Resnet：model_final_resnet50.pth\n",
    "learner.model.eval()\n",
    "print('learner loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_keys(model, pretrained_state_dict):\n",
    "    ckpt_keys = set(pretrained_state_dict.keys())\n",
    "    model_keys = set(model.state_dict().keys())\n",
    "    used_pretrained_keys = model_keys & ckpt_keys\n",
    "    unused_pretrained_keys = ckpt_keys - model_keys\n",
    "    missing_keys = model_keys - ckpt_keys\n",
    "    print('Missing keys:{}'.format(len(missing_keys)))\n",
    "    print('Unused checkpoint keys:{}'.format(len(unused_pretrained_keys)))\n",
    "    print('Used keys:{}'.format(len(used_pretrained_keys)))\n",
    "    assert len(used_pretrained_keys) > 0, 'load NONE from pretrained checkpoint'\n",
    "    return True\n",
    "\n",
    "\n",
    "def remove_prefix(state_dict, prefix):\n",
    "    ''' Old style model is stored with all names of parameters sharing common prefix 'module.' '''\n",
    "    print('remove prefix \\'{}\\''.format(prefix))\n",
    "    f = lambda x: x.split(prefix, 1)[-1] if x.startswith(prefix) else x\n",
    "    return {f(key): value for key, value in state_dict.items()}\n",
    "\n",
    "\n",
    "def load_model(model, pretrained_path, load_to_cpu):\n",
    "    print('Loading pretrained model from {}'.format(pretrained_path))\n",
    "    if load_to_cpu:\n",
    "        pretrained_dict = torch.load(pretrained_path, map_location=lambda storage, loc: storage)\n",
    "    else:\n",
    "        device = torch.cuda.current_device()\n",
    "        pretrained_dict = torch.load(pretrained_path, map_location=lambda storage, loc: storage.cuda(device))\n",
    "    if \"state_dict\" in pretrained_dict.keys():\n",
    "        pretrained_dict = remove_prefix(pretrained_dict['state_dict'], 'module.')\n",
    "    else:\n",
    "        pretrained_dict = remove_prefix(pretrained_dict, 'module.')\n",
    "    check_keys(model, pretrained_dict)\n",
    "    model.load_state_dict(pretrained_dict, strict=False)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference facial points, a list of coordinates (x,y)\n",
    "from skimage import transform as trans\n",
    "REFERENCE_FACIAL_POINTS = [\n",
    "    [30.29459953, 51.69630051],\n",
    "    [65.53179932, 51.50139999],\n",
    "    [48.02519989, 71.73660278],\n",
    "    [33.54930115, 92.3655014],\n",
    "    [62.72990036, 92.20410156]\n",
    "]\n",
    "\n",
    "DEFAULT_CROP_SIZE = (96, 112)\n",
    "def get_reference_facial_points(output_size=None,\n",
    "                                inner_padding_factor=0.0,\n",
    "                                outer_padding=(0, 0),\n",
    "                                default_square=False):\n",
    "    tmp_5pts = np.array(REFERENCE_FACIAL_POINTS)\n",
    "    tmp_crop_size = np.array(DEFAULT_CROP_SIZE)\n",
    "\n",
    "    # 0) make the inner region a square\n",
    "    if default_square:\n",
    "        size_diff = max(tmp_crop_size) - tmp_crop_size\n",
    "        tmp_5pts += size_diff / 2\n",
    "        tmp_crop_size += size_diff\n",
    "\n",
    "    # print('---> default:')\n",
    "    # print('              crop_size = ', tmp_crop_size)\n",
    "    # print('              reference_5pts = ', tmp_5pts)\n",
    "\n",
    "    if (output_size and\n",
    "            output_size[0] == tmp_crop_size[0] and\n",
    "            output_size[1] == tmp_crop_size[1]):\n",
    "        # print('output_size == DEFAULT_CROP_SIZE {}: return default reference points'.format(tmp_crop_size))\n",
    "        return tmp_5pts\n",
    "\n",
    "    if (inner_padding_factor == 0 and\n",
    "            outer_padding == (0, 0)):\n",
    "        if output_size is None:\n",
    "            print('No paddings to do: return default reference points')\n",
    "            return tmp_5pts\n",
    "        else:\n",
    "            raise FaceWarpException(\n",
    "                'No paddings to do, output_size must be None or {}'.format(tmp_crop_size))\n",
    "\n",
    "    # check output size\n",
    "    if not (0 <= inner_padding_factor <= 1.0):\n",
    "        raise FaceWarpException('Not (0 <= inner_padding_factor <= 1.0)')\n",
    "\n",
    "    if ((inner_padding_factor > 0 or outer_padding[0] > 0 or outer_padding[1] > 0)\n",
    "            and output_size is None):\n",
    "        output_size = tmp_crop_size * \\\n",
    "                      (1 + inner_padding_factor * 2).astype(np.int32)\n",
    "        output_size += np.array(outer_padding)\n",
    "        print('              deduced from paddings, output_size = ', output_size)\n",
    "\n",
    "    if not (outer_padding[0] < output_size[0]\n",
    "            and outer_padding[1] < output_size[1]):\n",
    "        raise FaceWarpException('Not (outer_padding[0] < output_size[0]'\n",
    "                                'and outer_padding[1] < output_size[1])')\n",
    "\n",
    "    # 1) pad the inner region according inner_padding_factor\n",
    "    # print('---> STEP1: pad the inner region according inner_padding_factor')\n",
    "    if inner_padding_factor > 0:\n",
    "        size_diff = tmp_crop_size * inner_padding_factor * 2\n",
    "        tmp_5pts += size_diff / 2\n",
    "        tmp_crop_size += np.round(size_diff).astype(np.int32)\n",
    "\n",
    "    # print('              crop_size = ', tmp_crop_size)\n",
    "    # print('              reference_5pts = ', tmp_5pts)\n",
    "\n",
    "    # 2) resize the padded inner region\n",
    "    # print('---> STEP2: resize the padded inner region')\n",
    "    size_bf_outer_pad = np.array(output_size) - np.array(outer_padding) * 2\n",
    "    # print('              crop_size = ', tmp_crop_size)\n",
    "    # print('              size_bf_outer_pad = ', size_bf_outer_pad)\n",
    "\n",
    "    if size_bf_outer_pad[0] * tmp_crop_size[1] != size_bf_outer_pad[1] * tmp_crop_size[0]:\n",
    "        raise FaceWarpException('Must have (output_size - outer_padding)'\n",
    "                                '= some_scale * (crop_size * (1.0 + inner_padding_factor)')\n",
    "\n",
    "    scale_factor = size_bf_outer_pad[0].astype(np.float32) / tmp_crop_size[0]\n",
    "    # print('              resize scale_factor = ', scale_factor)\n",
    "    tmp_5pts = tmp_5pts * scale_factor\n",
    "    #    size_diff = tmp_crop_size * (scale_factor - min(scale_factor))\n",
    "    #    tmp_5pts = tmp_5pts + size_diff / 2\n",
    "    tmp_crop_size = size_bf_outer_pad\n",
    "    # print('              crop_size = ', tmp_crop_size)\n",
    "    # print('              reference_5pts = ', tmp_5pts)\n",
    "\n",
    "    # 3) add outer_padding to make output_size\n",
    "    reference_5point = tmp_5pts + np.array(outer_padding)\n",
    "    tmp_crop_size = output_size\n",
    "    # print('---> STEP3: add outer_padding to make output_size')\n",
    "    # print('              crop_size = ', tmp_crop_size)\n",
    "    # print('              reference_5pts = ', tmp_5pts)\n",
    "    #\n",
    "    # print('===> end get_reference_facial_points\\n')\n",
    "\n",
    "    return reference_5point\n",
    "\n",
    "def get_affine_transform_matrix(src_pts, dst_pts):\n",
    "    tfm = np.float32([[1, 0, 0], [0, 1, 0]])\n",
    "    n_pts = src_pts.shape[0]\n",
    "    ones = np.ones((n_pts, 1), src_pts.dtype)\n",
    "    src_pts_ = np.hstack([src_pts, ones])\n",
    "    dst_pts_ = np.hstack([dst_pts, ones])\n",
    "\n",
    "    A, res, rank, s = np.linalg.lstsq(src_pts_, dst_pts_)\n",
    "\n",
    "    if rank == 3:\n",
    "        tfm = np.float32([\n",
    "            [A[0, 0], A[1, 0], A[2, 0]],\n",
    "            [A[0, 1], A[1, 1], A[2, 1]]\n",
    "        ])\n",
    "    elif rank == 2:\n",
    "        tfm = np.float32([\n",
    "            [A[0, 0], A[1, 0], 0],\n",
    "            [A[0, 1], A[1, 1], 0]\n",
    "        ])\n",
    "\n",
    "    return tfm\n",
    "\n",
    "\n",
    "def warp_and_crop_face(src_img,  # BGR\n",
    "                       facial_pts,\n",
    "                       reference_pts=None,\n",
    "                       crop_size=(96, 112),\n",
    "                       align_type='smilarity'):\n",
    "    if reference_pts is None:\n",
    "        if crop_size[0] == 96 and crop_size[1] == 112:\n",
    "            reference_pts = REFERENCE_FACIAL_POINTS\n",
    "        else:\n",
    "            default_square = False\n",
    "            inner_padding_factor = 0\n",
    "            outer_padding = (0, 0)\n",
    "            output_size = crop_size\n",
    "\n",
    "            reference_pts = get_reference_facial_points(output_size,\n",
    "                                                        inner_padding_factor,\n",
    "                                                        outer_padding,\n",
    "                                                        default_square)\n",
    "\n",
    "    ref_pts = np.float32(reference_pts)\n",
    "    ref_pts_shp = ref_pts.shape  # (5,2)\n",
    "    if max(ref_pts_shp) < 3 or min(ref_pts_shp) != 2:\n",
    "        raise FaceWarpException(\n",
    "            'reference_pts.shape must be (K,2) or (2,K) and K>2')\n",
    "\n",
    "    if ref_pts_shp[0] == 2:\n",
    "        ref_pts = ref_pts.T\n",
    "\n",
    "    src_pts = np.float32(facial_pts)\n",
    "    src_pts_shp = src_pts.shape   # (5,2)\n",
    "    if max(src_pts_shp) < 3 or min(src_pts_shp) != 2:\n",
    "        raise FaceWarpException(\n",
    "            'facial_pts.shape must be (K,2) or (2,K) and K>2')\n",
    "\n",
    "    if src_pts_shp[0] == 2:\n",
    "        src_pts = src_pts.T\n",
    "\n",
    "    if src_pts.shape != ref_pts.shape:\n",
    "        raise FaceWarpException(\n",
    "            'facial_pts and reference_pts must have the same shape')\n",
    "\n",
    "    if align_type is 'cv2_affine':\n",
    "        tfm = cv2.getAffineTransform(src_pts[0:3], ref_pts[0:3])\n",
    "    #        print('cv2.getAffineTransform() returns tfm=\\n' + str(tfm))\n",
    "    elif align_type is 'affine':\n",
    "        tfm = get_affine_transform_matrix(src_pts, ref_pts)\n",
    "    #        print('get_affine_transform_matrix() returns tfm=\\n' + str(tfm))\n",
    "    else:\n",
    "        # tfm = get_similarity_transform_for_cv2(src_pts, ref_pts)\n",
    "        tform = trans.SimilarityTransform()\n",
    "        tform.estimate(src_pts, ref_pts)\n",
    "        tfm = tform.params[0:2, :]\n",
    "\n",
    "    face_img = cv2.warpAffine(src_img, tfm, (crop_size[0], crop_size[1]))\n",
    "\n",
    "    return face_img  # BGR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained model from retinaface/weights/mobilenet0.25_Final.pth\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'retinaface/weights/mobilenet0.25_Final.pth'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-d02f024f2e46>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mcfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcfg_re50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mretina_net\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRetinaFace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mretina_net\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretina_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrained_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mretina_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#     print('Finished loading model!')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-750007f9e33c>\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(model, pretrained_path, load_to_cpu)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mpretrained_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m\"state_dict\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpretrained_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mpretrained_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremove_prefix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'module.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pthmodels/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'retinaface/weights/mobilenet0.25_Final.pth'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# 模型和device\n",
    "# net and model\n",
    "torch.set_grad_enabled(False)\n",
    "cfg = None\n",
    "if network == \"mobile0.25\":\n",
    "    cfg = cfg_mnet\n",
    "elif network == \"resnet50\":\n",
    "    cfg = cfg_re50\n",
    "retina_net = RetinaFace(cfg=cfg, phase='test')\n",
    "retina_net = load_model(retina_net, trained_model, cpu)\n",
    "retina_net.eval()\n",
    "#     print('Finished loading model!')\n",
    "#     print(net)\n",
    "cudnn.benchmark = True\n",
    "device = torch.device(\"cpu\" if cpu else \"cuda:0\")\n",
    "retina_net = retina_net.to(device)\n",
    "\n",
    "# Model parameters\n",
    "image_w = 112\n",
    "image_h = 112\n",
    "channel = 3\n",
    "emb_size = 512\n",
    "# 执行人脸对齐\n",
    "def align_face(img, facial5points):\n",
    "    facial5points = np.reshape(facial5points, (2, 5))\n",
    "    crop_size = (image_h, image_w)\n",
    "    default_square = True\n",
    "    inner_padding_factor = 0.25\n",
    "    outer_padding = (0, 0)\n",
    "    output_size = (image_h, image_w)\n",
    "#     print('output_size',output_size)\n",
    "    # get the reference 5 landmarks position in the crop settings\n",
    "    reference_5pts = get_reference_facial_points(output_size, inner_padding_factor, outer_padding, default_square)\n",
    "#     print('reference_5pts',reference_5pts)\n",
    "    # dst_img = warp_and_crop_face(raw, facial5points)\n",
    "    dst_img = warp_and_crop_face(img, facial5points, reference_pts=reference_5pts, crop_size=crop_size)\n",
    "#     print('dst_img',dst_img.shape)\n",
    "    return dst_img\n",
    "\n",
    "\n",
    "\n",
    "def detect_face(img=None,input_image_path = \"./curve/test.jpg\",output_image_path = 'test.jpg',net=retina_net,device=device):\n",
    "    \"\"\"\n",
    "    img 输入GBR格式图片矩阵\n",
    "    input_image_path 输入图片路径\n",
    "    output_image_path 保存图片路径\n",
    "    \"\"\"\n",
    "    resize = 1\n",
    "    if isinstance(input_image_path,str) and img is None:\n",
    "        img_raw = cv2.imread(input_image_path, cv2.IMREAD_COLOR)\n",
    "        img = np.float32(img_raw)\n",
    "    else:\n",
    "        img_raw = img\n",
    "    im_height, im_width, _ = img.shape\n",
    "    scale = torch.Tensor([img.shape[1], img.shape[0], img.shape[1], img.shape[0]])\n",
    "    img -= (104, 117, 123)\n",
    "    img = img.transpose(2, 0, 1)\n",
    "    img = torch.from_numpy(img).unsqueeze(0)\n",
    "    img = img.to(device)\n",
    "    scale = scale.to(device)\n",
    "\n",
    "    tic = time.time()\n",
    "    loc, conf, landms = net(img)  # forward pass\n",
    "    print('net forward time: {:.4f}'.format(time.time() - tic))\n",
    "\n",
    "    priorbox = PriorBox(cfg, image_size=(im_height, im_width))\n",
    "    priors = priorbox.forward()\n",
    "    priors = priors.to(device)\n",
    "    prior_data = priors.data\n",
    "    boxes = decode(loc.data.squeeze(0), prior_data, cfg['variance'])\n",
    "    boxes = boxes * scale / resize\n",
    "    boxes = boxes.cpu().numpy()\n",
    "    scores = conf.squeeze(0).data.cpu().numpy()[:, 1]\n",
    "    landms = decode_landm(landms.data.squeeze(0), prior_data, cfg['variance'])\n",
    "    scale1 = torch.Tensor([img.shape[3], img.shape[2], img.shape[3], img.shape[2],\n",
    "                           img.shape[3], img.shape[2], img.shape[3], img.shape[2],\n",
    "                           img.shape[3], img.shape[2]])\n",
    "    scale1 = scale1.to(device)\n",
    "    landms = landms * scale1 / resize\n",
    "    landms = landms.cpu().numpy()\n",
    "\n",
    "    # ignore low scores\n",
    "    inds = np.where(scores > confidence_threshold)[0]\n",
    "    boxes = boxes[inds]\n",
    "    landms = landms[inds]\n",
    "    scores = scores[inds]\n",
    "\n",
    "    # keep top-K before NMS\n",
    "    order = scores.argsort()[::-1][:top_k]\n",
    "    boxes = boxes[order]\n",
    "    landms = landms[order]\n",
    "    scores = scores[order]\n",
    "\n",
    "    # do NMS\n",
    "    dets = np.hstack((boxes, scores[:, np.newaxis])).astype(np.float32, copy=False)\n",
    "    keep = py_cpu_nms(dets, nms_threshold)\n",
    "    # keep = nms(dets, args.nms_threshold,force_cpu=args.cpu)\n",
    "    dets = dets[keep, :]\n",
    "    landms = landms[keep]\n",
    "\n",
    "    # keep top-K faster NMS\n",
    "    dets = dets[:keep_top_k, :]\n",
    "    landms = landms[:keep_top_k, :]\n",
    "\n",
    "    dets = np.concatenate((dets, landms), axis=1)\n",
    "    \n",
    "    # show image\n",
    "    for b in dets:  # b 0~3 为xmin ymin xmax ymax 4 为threshold  后面连续五个点为地标\n",
    "        if b[4] < vis_thres:\n",
    "            continue\n",
    "        # 仅处理一张图片只有一个人脸的情况\n",
    "        try:\n",
    "            crop_image =img_raw[int(b[1]):int(b[3]),int(b[0]):int(b[2]),:]\n",
    "            \n",
    "#             resize_crop_image = cv2.resize(crop_image,(112,112))\n",
    "            # 执行人脸对齐 point 按照 x1 x2 x3 x4 x5 y1 y2 y3 y4 y5\n",
    "            point  =[int(i) for i in [b[5]-b[0],b[7]-b[0],b[9]-b[0], b[11]-b[0],b[13]-b[0],b[6]-b[1],b[8]-b[1],b[10]-b[1],b[12]-b[1],b[14]-b[1]]]\n",
    "#             point  =[int(i) for i in [b[5]-b[0], b[6]-b[1],b[7]-b[0], b[8]-b[1], b[9]-b[0],b[10]-b[1], b[11]-b[0], b[12]-b[1], b[13]-b[0],b[14]-b[1]]]\n",
    "#             return crop_image ,point\n",
    "            facial5points = np.array(point)\n",
    "#             print(crop_image.shape,facial5points)\n",
    "            resize_crop_image = align_face(crop_image, facial5points)\n",
    "            return resize_crop_image\n",
    "        except:\n",
    "            print('exception')\n",
    "            return None\n",
    "        text = \"{:.4f}\".format(b[4])\n",
    "        b = list(map(int, b))\n",
    "        cv2.rectangle(img_raw, (b[0], b[1]), (b[2], b[3]), (0, 0, 255), 2)\n",
    "        cx = b[0]\n",
    "        cy = b[1] + 12\n",
    "        cv2.putText(img_raw, text, (cx, cy),\n",
    "                    cv2.FONT_HERSHEY_DUPLEX, 0.5, (255, 255, 255))\n",
    "\n",
    "        # landms\n",
    "        cv2.circle(img_raw, (b[5], b[6]), 1, (0, 0, 255), 4)\n",
    "        cv2.circle(img_raw, (b[7], b[8]), 1, (0, 255, 255), 4)\n",
    "        cv2.circle(img_raw, (b[9], b[10]), 1, (255, 0, 255), 4)\n",
    "        cv2.circle(img_raw, (b[11], b[12]), 1, (0, 255, 0), 4)\n",
    "        cv2.circle(img_raw, (b[13], b[14]), 1, (255, 0, 0), 4)\n",
    "\n",
    "    # save image\n",
    "    if save_image:\n",
    "        cv2.imwrite(output_image_path, img_raw)\n",
    "        \n",
    "\n",
    "def prepare_my_facebank(conf, model,tta = True):\n",
    "    model.eval()\n",
    "    embeddings =  []\n",
    "    names = ['Unknown']  # 留一个位置给unkn\n",
    "    for folder_name in os.listdir(prepare_image_path):\n",
    "        folder_path = os.path.join(prepare_image_path,folder_name)\n",
    "        if os.path.isfile(folder_path):  # 若是文件则跳过  应该按名称命名的文件夹\n",
    "            continue\n",
    "        else:\n",
    "            embs = []\n",
    "            for image_name in os.listdir(folder_path):\n",
    "                image_full_path = os.path.join(folder_path,image_name)\n",
    "                if not os.path.isfile(image_full_path):\n",
    "                    continue\n",
    "                else:\n",
    "                    face_image = detect_face(input_image_path=image_full_path)  # 返回的是GBR 格式图像 \n",
    "                    if face_image is None:\n",
    "                        continue\n",
    "                    img = face_image[:,:,[2,1,0]]\n",
    "#                     img=np.transpose(face_image,(2,0,1))\n",
    "                    with torch.no_grad():\n",
    "                        if tta:\n",
    "                            mirror = trans.functional.hflip(img)\n",
    "                            emb = model(conf.test_transform(img).to(conf.device).unsqueeze(0))\n",
    "                            emb_mirror = model(conf.test_transform(mirror).to(conf.device).unsqueeze(0))\n",
    "                            embs.append(l2_norm(emb + emb_mirror))\n",
    "                        else:                        \n",
    "                            embs.append(model(conf.test_transform(img).to(conf.device).unsqueeze(0)))\n",
    "        if len(embs) == 0:\n",
    "            continue\n",
    "        embedding = torch.cat(embs).mean(0,keepdim=True)\n",
    "        embeddings.append(embedding)\n",
    "        names.append(folder_name)\n",
    "    embeddings = torch.cat(embeddings)\n",
    "    names = np.array(names)\n",
    "    torch.save(embeddings, os.path.join(prepare_image_path,'facebank.pth'))\n",
    "    np.save(os.path.join(prepare_image_path,'name.npy'), names)\n",
    "\n",
    "    return embeddings, names\n",
    "\n",
    "def load_my_facebank(conf):\n",
    "    embeddings = torch.load(os.path.join(prepare_image_path,'facebank.pth'))\n",
    "    names = np.load(os.path.join(prepare_image_path,'name.npy'))\n",
    "    return embeddings, names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 更新人脸字典  或加载已存在字典\n",
    "update=True\n",
    "if update:\n",
    "    targets, names = prepare_my_facebank(conf, learner.model,tta = False)\n",
    "    print('facebank updated')\n",
    "else:\n",
    "    targets, names = load_my_facebank(conf)\n",
    "    print('facebank loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在图片上测试\n",
    "test_image_path='/home/create/jing/jing_vision/face/image/aqgy/val/lzq/ia_1700002438.jpg'\n",
    "save_image_path = '/Data/jing/face/facebank/tyy/ia_1900002420_test.jpg'\n",
    "img_raw = cv2.imread(test_image_path)\n",
    "test_face_image = detect_face(input_image_path=test_image_path)  # 返回的是GBR 格式图像 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在图片上测试\n",
    "test_image_path='/home/create/jing/jing_vision/face/image/aqgy/val/lzq/ia_1700002438.jpg'\n",
    "save_image_path = '/Data/jing/face/facebank/tyy/ia_1900002420_test.jpg'\n",
    "test_face_image = detect_face(input_image_path=test_image_path)  # 返回的是GBR 格式图像 \n",
    "test_img = test_face_image[:,:,[2,1,0]]\n",
    "encode = learner.model(conf.test_transform(test_img).to(conf.device).unsqueeze(0))\n",
    "b = encode.squeeze().cpu().numpy()\n",
    "distance_list=[0]*15\n",
    "for i in range(targets.shape[0]):\n",
    "    a = targets[i,:].cpu().numpy()\n",
    "    distance_list[i]=np.sqrt(np.sum(np.square(a-b)))\n",
    "    \n",
    "#     distance_list[i] = 1 - np.dot(a,b)/(np.linalg.norm(a)*np.linalg.norm(b))\n",
    "print(distance_list)\n",
    "print(names[np.argmin(distance_list)+1])  #+1的原因是第一个留给unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在测试集上测试准确率\n",
    "test_path = '/home/create/jing/jing_vision/face/image/5-celebrity-faces-dataset/data/val'\n",
    "all_count = 0\n",
    "currect_count = 0\n",
    "for name in os.listdir(test_path):\n",
    "    print('-------------',name)\n",
    "    name_folder = os.path.join(test_path,name)\n",
    "    for image_name in os.listdir(name_folder):\n",
    "        full_image_path = os.path.join(name_folder,image_name)\n",
    "        test_face_image = detect_face(input_image_path=full_image_path)  # 返回的是GBR 格式图像 \n",
    "        test_img = test_face_image[:,:,[2,1,0]]\n",
    "        encode = learner.model(conf.test_transform(test_img).to(conf.device).unsqueeze(0))\n",
    "        b = encode.squeeze().cpu().numpy()\n",
    "        distance_list=[0]*15\n",
    "        for i in range(targets.shape[0]):\n",
    "            a = targets[i,:].cpu().numpy()\n",
    "            distance_list[i]=np.sqrt(np.sum(np.square(a-b)))\n",
    "#             print(distance_list)\n",
    "        precit_name = names[np.argmin(distance_list)+1]\n",
    "        print(precit_name)  #+1的原因是第一个留给unknown\n",
    "        if (name==precit_name):\n",
    "            currect_count +=1\n",
    "        all_count+=1\n",
    "print(\"Accuracy:\",currect_count/all_count,all_count,currect_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在测试集上测试准确率\n",
    "test_path = '/home/create/jing/jing_vision/face/image/aqgy/val/'\n",
    "all_count = 0\n",
    "currect_count = 0\n",
    "for name in os.listdir(test_path):\n",
    "    print('-------------',name)\n",
    "    name_folder = os.path.join(test_path,name)\n",
    "    for image_name in os.listdir(name_folder):\n",
    "        full_image_path = os.path.join(name_folder,image_name)\n",
    "        test_face_image = detect_face(input_image_path=full_image_path)  # 返回的是GBR 格式图像 \n",
    "        test_img = test_face_image[:,:,[2,1,0]]\n",
    "        encode = learner.model(conf.test_transform(test_img).to(conf.device).unsqueeze(0))\n",
    "        b = encode.squeeze().cpu().numpy()\n",
    "        distance_list=[0]*15\n",
    "        for i in range(targets.shape[0]):\n",
    "            a = targets[i,:].cpu().numpy()\n",
    "            distance_list[i]=np.sqrt(np.sum(np.square(a-b)))\n",
    "#             print(distance_list)\n",
    "        precit_name = names[np.argmin(distance_list)+1]\n",
    "        print(precit_name)  #+1的原因是第一个留给unknown\n",
    "        if (name==precit_name):\n",
    "            currect_count +=1\n",
    "        all_count+=1\n",
    "print(\"Accuracy:\",currect_count/all_count,all_count,currect_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_video_name='/home/create/jing/jing_vision/face/aqgy.flv'\n",
    "out_video_name = '/home/create/jing/jing_vision/face/aqgy_out_insight.avi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videoCapture = cv2.VideoCapture(input_video_name)\n",
    "fps = int(videoCapture.get(cv2.CAP_PROP_FPS))\n",
    "w = videoCapture.get(3)\n",
    "h = videoCapture.get(4)\n",
    "print('w,h',w,h)\n",
    "print('fps:',fps)\n",
    "fourcc_1 = cv2.VideoWriter_fourcc('M', 'J', 'P', 'G')  # opencv3.0\n",
    "size = (int(w),int(h))\n",
    "# 指定写视频的格式, I420-avi, MJPG-mp4\n",
    "write_video_path = out_video_name\n",
    "print('写入视频路劲：',write_video_path)\n",
    "videoWriter = cv2.VideoWriter(write_video_path, fourcc_1, int(fps), size)\n",
    "frame_count=0\n",
    "while True:\n",
    "    start_time = time.time()\n",
    "    # print('当前帧:', fream_count)\n",
    "    rval, frame = videoCapture.read()\n",
    "    if rval is False:\n",
    "        print('video is over')\n",
    "        break\n",
    "    if frame_count%(2)==0:    #int(fps)*7\n",
    "#         im = Image.fromarray(np.uint8(frame[:,:,[2,1,0]]))\n",
    "        im = np.float32(frame[:,:,[2,1,0]])\n",
    "        test_face_image = detect_face(img=im,input_image_path=None)  # 返回的是GBR 格式图像 \n",
    "        if test_face_image is None:\n",
    "            continue\n",
    "        test_img = test_face_image[:,:,[2,1,0]]\n",
    "        encode = learner.model(conf.test_transform(test_img).to(conf.device).unsqueeze(0))\n",
    "        b = encode.squeeze().cpu().numpy()\n",
    "        distance_list=[0]*10\n",
    "        for i in range(targets.shape[0]):\n",
    "            a = targets[i,:].cpu().numpy()\n",
    "            distance_list[i]=np.sqrt(np.sum(np.square(a-b)))\n",
    "#         print(distance_list)\n",
    "        name = names[np.argmin(distance_list)+1]  #+1的原因是第一个留给unknown\n",
    "        \n",
    "        cv2.putText(frame, name, (30, 80), cv2.FONT_HERSHEY_SIMPLEX, 2, (0, 0, 255), 5)\n",
    "        \n",
    "        process_frame_time = time.time()\n",
    "        print('处理帧fps:',1/(process_frame_time-start_time))\n",
    "        # cv2.imshow(\"Frame\", frame)\n",
    "        # cv2.waitKey(1)\n",
    "    videoWriter.write(frame)\n",
    "    frame_count += 1\n",
    "videoCapture.release()\n",
    "videoWriter.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pthmodels]",
   "language": "python",
   "name": "conda-env-pthmodels-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
